# litellm-config.yaml
# See all possible litellm settings here: https://docs.litellm.ai/docs/configuration

litellm_settings:
  set_verbose: true
  drop_params: true # Drop params not supported by the model
  
# Router configuration enables routing to multiple models/deployments
router_settings:
  enable_failovers: true
  # Set a default routing behavior
  routing_strategy: "simple-shuffle" # "latency-based" and "usage-based" are also options

model_list:
  # OpenAI
  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: env.OPENAI_API_KEY
  - model_name: gpt-4-turbo
    litellm_params:
      model: openai/gpt-4-turbo
      api_key: env.OPENAI_API_KEY

  # Groq - Fast inference
  - model_name: llama3-8b-8192
    litellm_params:
      model: groq/llama3-8b-8192
      api_key: env.GROQ_API_KEY
  - model_name: gemma-7b-it
    litellm_params:
      model: groq/gemma-7b-it
      api_key: env.GROQ_API_KEY
  
  # Anthropic
  - model_name: claude-3-opus-20240229
    litellm_params:
      model: anthropic/claude-3-opus-20240229
      api_key: env.ANTHROPIC_API_KEY

  # Ollama (if you enable the service in docker-compose.yml)
  # - model_name: ollama/llama3
  #   litellm_params:
  #     model: ollama/llama3
  #     api_base: "http://ollama:11434" 
